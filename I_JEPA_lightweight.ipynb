{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+uJUn/0/wfEv4J/68BV/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b97386bdf0ee49e6af1671f447e748bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de51091437174ef7aca870a3678db66c",
              "IPY_MODEL_8b70e37994a242e08b2395553c918974",
              "IPY_MODEL_51c2499594ac48c582a38aaf7e05cb53"
            ],
            "layout": "IPY_MODEL_e774bae84bad4084bd35b768f6c97361"
          }
        },
        "de51091437174ef7aca870a3678db66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d41c95419040a78c2210253e50330f",
            "placeholder": "​",
            "style": "IPY_MODEL_3e42f292b60e49789885bde7ae19ca3c",
            "value": "model.safetensors: 100%"
          }
        },
        "8b70e37994a242e08b2395553c918974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f707d838a31e42e09693edd0ca31cbcf",
            "max": 22883348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_940d1e0fbce7450b9aacdaa6dc1f5483",
            "value": 22883348
          }
        },
        "51c2499594ac48c582a38aaf7e05cb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbab85b8e0644247a4367fb5aea9dee1",
            "placeholder": "​",
            "style": "IPY_MODEL_d457ea4a6cbe421994e2150775f37631",
            "value": " 22.9M/22.9M [00:00&lt;00:00, 98.7MB/s]"
          }
        },
        "e774bae84bad4084bd35b768f6c97361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d41c95419040a78c2210253e50330f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e42f292b60e49789885bde7ae19ca3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f707d838a31e42e09693edd0ca31cbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940d1e0fbce7450b9aacdaa6dc1f5483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbab85b8e0644247a4367fb5aea9dee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d457ea4a6cbe421994e2150775f37631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laere11/machine_learning/blob/main/I_JEPA_lightweight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an updated code snippet that uses the lightweight model variant vit_tiny_patch16_224 and trains on a subset of 5,000 CIFAR-10 images. It also includes checkpointing so that the best model is saved to the /content directory."
      ],
      "metadata": {
        "id": "UyJs5vGRdsxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Had to change to the lightweight model because the original code was going to take 104 hours of compute time to go though the 3 epochs.  because it was referencing 50,000 immages and had alot of parameters.   this model has less parameters and the training data set has been trimmed to 5000, plus I limited it to run only 2 epochs which I hope will be less than 2 hours of compute time using the T4 GPU"
      ],
      "metadata": {
        "id": "qx2Es_Knd5cZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871,
          "referenced_widgets": [
            "b97386bdf0ee49e6af1671f447e748bf",
            "de51091437174ef7aca870a3678db66c",
            "8b70e37994a242e08b2395553c918974",
            "51c2499594ac48c582a38aaf7e05cb53",
            "e774bae84bad4084bd35b768f6c97361",
            "a0d41c95419040a78c2210253e50330f",
            "3e42f292b60e49789885bde7ae19ca3c",
            "f707d838a31e42e09693edd0ca31cbcf",
            "940d1e0fbce7450b9aacdaa6dc1f5483",
            "fbab85b8e0644247a4367fb5aea9dee1",
            "d457ea4a6cbe421994e2150775f37631"
          ]
        },
        "id": "EYR-hsg7diqZ",
        "outputId": "534751d6-360f-49ca-a4ed-e6a91ba14fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b97386bdf0ee49e6af1671f447e748bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 10] loss: 13.8504\n",
            "[Epoch 1, Batch 20] loss: 12.5808\n",
            "[Epoch 1, Batch 30] loss: 11.8690\n",
            "[Epoch 1, Batch 40] loss: 11.1519\n",
            "[Epoch 1, Batch 50] loss: 10.4092\n",
            "[Epoch 1, Batch 60] loss: 9.7788\n",
            "[Epoch 1, Batch 70] loss: 9.2318\n",
            "[Epoch 1, Batch 80] loss: 8.8164\n",
            "[Epoch 1, Batch 90] loss: 8.3376\n",
            "[Epoch 1, Batch 100] loss: 7.9416\n",
            "[Epoch 1, Batch 110] loss: 7.7101\n",
            "[Epoch 1, Batch 120] loss: 7.3959\n",
            "[Epoch 1, Batch 130] loss: 7.1429\n",
            "[Epoch 1, Batch 140] loss: 6.8416\n",
            "[Epoch 1, Batch 150] loss: 6.5556\n",
            "Epoch 1 completed with average loss: 0.2858\n",
            "Checkpoint saved at epoch 1 with loss 0.2858\n",
            "Finished epoch 1/2\n",
            "[Epoch 2, Batch 10] loss: 6.3263\n",
            "[Epoch 2, Batch 20] loss: 6.0779\n",
            "[Epoch 2, Batch 30] loss: 5.9936\n",
            "[Epoch 2, Batch 40] loss: 5.8667\n",
            "[Epoch 2, Batch 50] loss: 5.8150\n",
            "[Epoch 2, Batch 60] loss: 5.6869\n",
            "[Epoch 2, Batch 70] loss: 5.7189\n",
            "[Epoch 2, Batch 80] loss: 5.6255\n",
            "[Epoch 2, Batch 90] loss: 5.5931\n",
            "[Epoch 2, Batch 100] loss: 5.5109\n",
            "[Epoch 2, Batch 110] loss: 5.5208\n",
            "[Epoch 2, Batch 120] loss: 5.4665\n",
            "[Epoch 2, Batch 130] loss: 5.5477\n",
            "[Epoch 2, Batch 140] loss: 5.5442\n",
            "[Epoch 2, Batch 150] loss: 5.4596\n",
            "Epoch 2 completed with average loss: 0.2470\n",
            "Checkpoint saved at epoch 2 with loss 0.2470\n",
            "Finished epoch 2/2\n",
            "Training finished\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Define the I-JEPA model using a lightweight ViT backbone (vit_tiny_patch16_224)\n",
        "class SimpleIJEPAModel(nn.Module):\n",
        "    def __init__(self, model_name='vit_tiny_patch16_224', mask_ratio=0.5, ema_decay=0.99):\n",
        "        super().__init__()\n",
        "        # Create the student ViT backbone\n",
        "        self.student = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
        "        # Create the teacher as a deepcopy of the student and freeze its parameters\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        for param in self.teacher.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Delete teacher.pos_embed to align with student\n",
        "        if hasattr(self.teacher, 'pos_embed'):\n",
        "            del self.teacher.pos_embed\n",
        "\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "        # A learnable mask token\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, self.student.embed_dim))\n",
        "        # A simple predictor head: an MLP\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(self.student.embed_dim, self.student.embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.student.embed_dim, self.student.embed_dim)\n",
        "        )\n",
        "        # Register the positional embeddings as a buffer and then delete the original from student\n",
        "        self.register_buffer('pos_embed', self.student.pos_embed[:, 1:, :])\n",
        "        del self.student.pos_embed\n",
        "\n",
        "    def update_teacher(self):\n",
        "        # Update teacher using exponential moving average (EMA)\n",
        "        for student_param, teacher_param in zip(self.student.parameters(), self.teacher.parameters()):\n",
        "            teacher_param.data = self.ema_decay * teacher_param.data + (1 - self.ema_decay) * student_param.data\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        # Compute patch embeddings\n",
        "        x_patches = self.student.patch_embed(x)\n",
        "        x_patches = x_patches + self.pos_embed\n",
        "        N = x_patches.size(1)\n",
        "        num_mask = int(self.mask_ratio * N)\n",
        "        mask = torch.zeros(B, N, dtype=torch.bool, device=x.device)\n",
        "        for i in range(B):\n",
        "            perm = torch.randperm(N, device=x.device)\n",
        "            mask[i, perm[:num_mask]] = True\n",
        "\n",
        "        # Replace masked tokens with the mask token\n",
        "        student_tokens = x_patches.clone()\n",
        "        student_tokens[mask] = self.mask_token\n",
        "\n",
        "        # Pass tokens through student's transformer blocks\n",
        "        for blk in self.student.blocks:\n",
        "            student_tokens = blk(student_tokens)\n",
        "        student_tokens = self.student.norm(student_tokens)\n",
        "        student_pred = self.predictor(student_tokens)\n",
        "\n",
        "        # Teacher processes full image (without masking)\n",
        "        with torch.no_grad():\n",
        "            teacher_tokens = self.teacher.patch_embed(x) + self.pos_embed\n",
        "            for blk in self.teacher.blocks:\n",
        "                teacher_tokens = blk(teacher_tokens)\n",
        "            teacher_tokens = self.teacher.norm(teacher_tokens)\n",
        "        # Compute loss over the masked tokens\n",
        "        loss = ((student_pred[mask] - teacher_tokens[mask]) ** 2).mean()\n",
        "        return loss\n",
        "\n",
        "def train_with_checkpoint(num_epochs=3, checkpoint_path=\"/content/best_model_checkpoint.pth\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Define transforms for CIFAR-10 (resize to 224x224 to match ViT input)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Download CIFAR-10 training set and use a subset (first 5000 images)\n",
        "    full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    subset_indices = list(range(5000))  # use 5000 images for quicker training\n",
        "    trainset = Subset(full_trainset, subset_indices)\n",
        "\n",
        "    # Increase batch size if possible; here we use 32 for faster training\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "    model = SimpleIJEPAModel().to(device)\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        num_batches = 0\n",
        "        for i, (inputs, _) in enumerate(trainloader):\n",
        "            inputs = inputs.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            model.update_teacher()\n",
        "            running_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            if (i + 1) % 10 == 0:\n",
        "                avg_loss = running_loss / 10\n",
        "                print(f\"[Epoch {epoch+1}, Batch {i+1}] loss: {avg_loss:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Compute a rough epoch loss (could also add a proper validation step)\n",
        "        epoch_loss = running_loss / max(num_batches, 1)\n",
        "        print(f\"Epoch {epoch+1} completed with average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if improved\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1} with loss {best_loss:.4f}\")\n",
        "\n",
        "        print(f\"Finished epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    print(\"Training finished\")\n",
        "    return model\n",
        "\n",
        "# Run training with checkpointing using the tiny model and a subset of 5000 images\n",
        "model = train_with_checkpoint(num_epochs=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model variable is defined, you can run the verification code. Here's the complete snippet with a reminder to define or load your model:"
      ],
      "metadata": {
        "id": "C-bWsYARhpiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run a short verification script that feeds a small batch of test images into your trained model to check that it’s accessible and producing outputs as expected. For example, the code below loads a few CIFAR-10 images, passes them through the model in evaluation mode, and prints out the computed loss as well as the shape of the student’s patch embeddings. This will help confirm that the model has been trained and that you can use it in downstream tasks."
      ],
      "metadata": {
        "id": "2vfEqt_fiN-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def verify_trained_model(model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Use the same transforms as during training\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    # Load a few test images (using CIFAR-10 test set)\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    dataloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # Get a single batch of test images\n",
        "    inputs, _ = next(iter(dataloader))\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # Run through the model; here the model returns a loss, so we just check it runs without error.\n",
        "    with torch.no_grad():\n",
        "        loss = model(inputs)\n",
        "        print(\"Verification loss on a test batch:\", loss.item())\n",
        "\n",
        "    # Additionally, check the output of the student network's patch embeddings.\n",
        "    with torch.no_grad():\n",
        "        patch_embeddings = model.student.patch_embed(inputs)\n",
        "        print(\"Shape of student patch embeddings:\", patch_embeddings.shape)\n",
        "\n",
        "# Make sure your model is defined or loaded before calling the verification function.\n",
        "# For example:\n",
        "# model = ... (your trained model)\n",
        "# Then run:\n",
        "verify_trained_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKRqmcOYg2Lx",
        "outputId": "57af30e1-171a-4cbb-9537-3e4152309b68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification loss on a test batch: 5.386413097381592\n",
            "Shape of student patch embeddings: torch.Size([8, 196, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that you have the model variable defined in your notebook (either by training it or loading a saved checkpoint) before calling verify_trained_model(model). This will confirm that the model is accessible and working as expected."
      ],
      "metadata": {
        "id": "xwr0RSofhvCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, its parameters (especially those of the student network) encode useful visual representations that you can leverage in various downstream tasks. Here are a few ways to utilize the trained model:\n",
        "\n",
        "Feature Extraction for Downstream Tasks:\n",
        "You can freeze the trained backbone (the student network) and use it to extract feature embeddings from images. These features can then be used for tasks such as:\n",
        "\n",
        "Image Classification: Attach a new classification head and fine-tune the entire model or just the head on a labeled dataset.\n",
        "\n",
        "Clustering or Retrieval: Use the feature vectors for clustering similar images or for similarity search.\n",
        "\n",
        "Object Detection or Segmentation: Integrate the backbone into a larger network for more complex vision tasks.\n",
        "\n",
        "Fine-Tuning for Specific Tasks:\n",
        "If you have a specific task in mind (e.g., CIFAR-10 classification), you can:\n",
        "\n",
        "Load the trained student network.\n",
        "\n",
        "Add a classification head (like a linear layer) on top of the features.\n",
        "\n",
        "Fine-tune the network (or only the classification head) on your labeled data.\n",
        "\n",
        "Below is an example code snippet demonstrating how to use the trained model's student network as a fixed feature extractor and then fine-tune a classifier on CIFAR-10:"
      ],
      "metadata": {
        "id": "bWReLZFPiX_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Assume SimpleIJEPAModel has been defined and trained already.\n",
        "# We'll use the student's backbone for feature extraction.\n",
        "\n",
        "class FineTuneClassifier(nn.Module):\n",
        "    def __init__(self, feature_extractor, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        # Freeze the feature extractor if desired\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # A simple classifier head\n",
        "        # We'll assume the output feature dimension is the same as the ViT embed_dim.\n",
        "        self.classifier = nn.Linear(self.feature_extractor.embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get patch embeddings from the feature extractor's patch embedding layer\n",
        "        # Note: You may need to adjust this if you want to pool the patch features.\n",
        "        x = self.feature_extractor.patch_embed(x)  # shape: (B, N, D)\n",
        "        # For simplicity, take the average over patch tokens as the image representation.\n",
        "        x = x.mean(dim=1)  # shape: (B, D)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Define data transformations for CIFAR-10 (resize to 224x224 to match ViT input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training and test datasets.\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming `model` is your trained SimpleIJEPAModel.\n",
        "# We'll use the student network from your trained model.\n",
        "# Make sure your trained model is loaded in the variable `model`.\n",
        "# For demonstration, we're assuming `model.student` is the feature extractor.\n",
        "\n",
        "feature_extractor = model.student\n",
        "feature_extractor.eval()  # set to eval mode\n",
        "\n",
        "classifier_model = FineTuneClassifier(feature_extractor, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Fine-tuning loop (for example, 5 epochs)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    classifier_model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "classifier_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = classifier_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpIQH5bfibKy",
        "outputId": "8cda57ea-c7ee-40d2-bd9a-9db5faf08339"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 2.1601\n",
            "Epoch 2/5 - Loss: 2.0769\n",
            "Epoch 3/5 - Loss: 2.0555\n",
            "Epoch 4/5 - Loss: 2.0442\n",
            "Epoch 5/5 - Loss: 2.0367\n",
            "Test Accuracy: 27.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Feature Extraction:\n",
        "The FineTuneClassifier class wraps the feature extractor (the trained student network) and adds a new linear layer for classification. In this example, the classifier simply averages the patch embeddings to form a global image feature, which may be sufficient for a quick demo. In practice, you might consider using a [CLS] token or more sophisticated pooling.\n",
        "\n",
        "Freezing and Fine-Tuning:\n",
        "You have the option to freeze the backbone during fine-tuning so that only the classifier head learns, or you can allow both to be fine-tuned for better performance (usually with a lower learning rate for the backbone).\n",
        "\n",
        "Evaluation:\n",
        "The code demonstrates a simple training loop for fine-tuning on CIFAR-10, followed by an evaluation loop that calculates test accuracy.\n",
        "\n",
        "This approach lets you leverage the self-supervised learning that occurred in the initial training. You can extend or modify this workflow for other tasks, such as object detection or semantic segmentation, by integrating the backbone into task-specific models."
      ],
      "metadata": {
        "id": "jCqXWM4Dicck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy result of 27.37% is low due to the abbeviated model training epoch count of 2 (set low for demonstration purposes only).  This code is example sucessfully completed all tasks.  Now that the code integrity has been validated the model could be trained more extensively and the feature extraction test accuracy results would increase accordingly.  "
      ],
      "metadata": {
        "id": "DDm-KpzLkhzJ"
      }
    }
  ]
}